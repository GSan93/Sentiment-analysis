{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Sentiment analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXSdgKF5CmZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import keras\n",
        "import sklearn\n",
        "import gensim\n",
        "import random\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import re\n",
        "from keras.preprocessing import text\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense , Dropout , Activation\n",
        "from keras.layers import Embedding , LSTM, BatchNormalization, SpatialDropout1D\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report \n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec , TaggedDocument\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# size of the word embeddings\n",
        "embeddings_dim = 100\n",
        "\n",
        "# maximum number of words to consider in the representations\n",
        "max_features = 15000\n",
        "\n",
        "# maximum length of a sentence\n",
        "max_sent_len = 20\n",
        "\n",
        "# percentage of the data used for model training\n",
        "percent = 0.80\n",
        "\n",
        "# number of classes\n",
        "num_classes = 4\n",
        "\n",
        "path_to_glove_embed = 'glove.6B.100d.txt'\n",
        "\n",
        "embeddings = dict( )\n",
        "\n",
        "data = [ ( row[\"single\"] , row[\"label\"]  ) for row in csv.DictReader(open(\"train.txt\",encoding='utf8'), delimiter='\\t', quoting=csv.QUOTE_NONE) ]\n",
        "random.shuffle( data )\n",
        "train_size = int(len(data) * percent)\n",
        "\n",
        "train_texts = []\n",
        "test_texts = []\n",
        "emo_train_texts = [ txt.lower() for ( txt, label ) in data[0:train_size] ]\n",
        "train_labels = [ label for ( txt , label ) in data[0:train_size] ]\n",
        "\n",
        "emo_test_texts = [ txt.lower() for ( txt, label ) in data[train_size:-1] ]\n",
        "test_labels = [ label for ( txt , label ) in data[train_size:-1] ]\n",
        "\n",
        "for i in emo_train_texts:\n",
        "    train_texts.append(re.sub(r'[-()_,.:@#?!&$]', ' ', emoji.demojize(i)))\n",
        "for i in emo_test_texts:\n",
        "    st = ''.join((x for x in emoji.demojize(i) if x not in string.punctuation))\n",
        "    test_texts.append(re.sub(r'[-()_,.:@#?!&$]', ' ', emoji.demojize(i)))\n",
        "    \n",
        "# print(train_texts)\n",
        "for i in range(0,len(train_labels)):\n",
        "    if train_labels[i] == \"happy\":\n",
        "        train_labels[i]=0\n",
        "    elif train_labels[i] == \"sad\":\n",
        "        train_labels[i]=1\n",
        "    elif train_labels[i] == \"angry\":\n",
        "        train_labels[i]=2\n",
        "    elif train_labels[i] == \"others\":\n",
        "        train_labels[i]=3\n",
        "\n",
        "for i in range(0,len(test_labels)):\n",
        "    if test_labels[i] == \"happy\":\n",
        "        test_labels[i]=0\n",
        "    elif test_labels[i] == \"sad\":\n",
        "        test_labels[i]=1\n",
        "    elif test_labels[i] == \"angry\":\n",
        "        test_labels[i]=2\n",
        "    elif test_labels[i] == \"others\":\n",
        "        test_labels[i]=3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKwtBhIZCmZY",
        "colab_type": "code",
        "colab": {},
        "outputId": "e8d4f3fc-4b3b-4471-f246-7272d6bc25af"
      },
      "source": [
        "embeddings =  gensim.models.Word2Vec(train_texts, min_count=1, size=300)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features,lower=True)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "train_sequences = sequence.pad_sequences( tokenizer.texts_to_sequences( train_texts ) , maxlen=max_sent_len )\n",
        "test_sequences = sequence.pad_sequences( tokenizer.texts_to_sequences( test_texts ) , maxlen=max_sent_len )\n",
        "print(train_sequences)\n",
        "train_matrix = tokenizer.texts_to_matrix( train_texts )\n",
        "test_matrix = tokenizer.texts_to_matrix( test_texts )\n",
        "embedding_weights = np.zeros( ( max_features , embeddings_dim ) )\n",
        "for word,index in tokenizer.word_index.items():\n",
        "  if index < max_features:\n",
        "    try: embedding_weights[index,:] = embeddings[word]\n",
        "    except: embedding_weights[index,:] = np.random.rand( 1 , embeddings_dim )\n",
        "\n",
        "le = preprocessing.LabelEncoder( )\n",
        "le.fit( train_labels + test_labels )\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                  np.unique(train_labels),\n",
        "                                                  train_labels)\n",
        "class_weights_dict = dict(zip(le.transform(list(le.classes_)),\n",
        "                              class_weights))\n",
        "\n",
        "train_labels = le.transform( train_labels )\n",
        "test_labels = le.transform( test_labels )\n",
        "print(\"Classi considerate: \" + repr( le.classes_ ))\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "\"\"\"pre-trained Glove\"\"\"\n",
        "embeddings_index = dict()\n",
        "\n",
        "\n",
        "f = open(\"glove.6B.300d.txt\", encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  373    6    2]\n",
            " [   0   35   13 ...   20   10   21]\n",
            " [   1   44  103 ...   30   80  243]\n",
            " ...\n",
            " [  15  171  205 ...   12  139  314]\n",
            " [   0    0    0 ...   31   11  193]\n",
            " [  38    2 2396 ...   38  321    3]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classi considerate: array([0, 1, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pC-DrRUYCmZg",
        "colab_type": "code",
        "colab": {},
        "outputId": "460f911e-edc9-42df-b980-d841701b78ea"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=20,trainable=False))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(64, activation='tanh', dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(64,activation='tanh', dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "adam=keras.optimizers.Adam(lr=0.01)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "print(\"---CLASS WEIGHTS---\")\n",
        "class_weights_dict[3] = 1.75\n",
        "print(class_weights_dict)\n",
        "\n",
        "model.fit(train_sequences, train_labels , epochs=5, batch_size=40, class_weight=class_weights_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_20 (Embedding)     (None, 20, 300)           3960600   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_20 (Spatia (None, 20, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_45 (LSTM)               (None, 20, 64)            93440     \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 20, 64)            256       \n",
            "_________________________________________________________________\n",
            "lstm_46 (LSTM)               (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 4)                 260       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 4,087,836\n",
            "Trainable params: 126,980\n",
            "Non-trainable params: 3,960,856\n",
            "_________________________________________________________________\n",
            "---CLASS WEIGHTS---\n",
            "{0: 1.766325036603221, 1: 1.3905025357307514, 2: 1.3650147092102285, 3: 1.75}\n",
            "Epoch 1/5\n",
            "24128/24128 [==============================] - 119s 5ms/step - loss: 0.4532 - acc: 0.8832\n",
            "Epoch 2/5\n",
            "24128/24128 [==============================] - 106s 4ms/step - loss: 0.3628 - acc: 0.9123\n",
            "Epoch 3/5\n",
            "24128/24128 [==============================] - 105s 4ms/step - loss: 0.3418 - acc: 0.9170\n",
            "Epoch 4/5\n",
            "24128/24128 [==============================] - 108s 4ms/step - loss: 0.3422 - acc: 0.9173\n",
            "Epoch 5/5\n",
            "24128/24128 [==============================] - 111s 5ms/step - loss: 0.3443 - acc: 0.9171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x20fcee82a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guviQIzaCmZl",
        "colab_type": "code",
        "colab": {},
        "outputId": "d259be55-c55f-43ee-c6bb-7231238a9b10"
      },
      "source": [
        "results = model.predict_classes( test_sequences )\n",
        "test_labels = list(test_labels)\n",
        "results = list(results)\n",
        "# print(test_labels,results)\n",
        "print (\"confusion_matrix : \")\n",
        "print (confusion_matrix(test_labels, results))\n",
        "print (\"classification_report: \")\n",
        "print (classification_report(test_labels, results))\n",
        "count = 0\n",
        "f = open(\"results.txt\", \"w+\",encoding=\"utf8\", newline=\"\\n\")\n",
        "for i in range(0,len(results)):\n",
        "    if test_labels[i] == results[i]:\n",
        "        count = count+1;\n",
        "    if test_labels[i] == 0:\n",
        "        test_labels[i]=\"happy\"\n",
        "    elif test_labels[i] == 1:\n",
        "        test_labels[i]= \"sad\"\n",
        "    elif test_labels[i] == 2:\n",
        "        test_labels[i]= \"angry\"\n",
        "    elif test_labels[i] == 3:\n",
        "        test_labels[i]= \"others\" \n",
        "    if results[i] == 0:\n",
        "        results[i]=\"happy\"\n",
        "    elif results[i] == 1:\n",
        "        results[i]= \"sad\"\n",
        "    elif results[i] == 2:\n",
        "        results[i]= \"angry\"\n",
        "    elif results[i] == 3:\n",
        "        results[i]= \"others\" \n",
        "    \n",
        "    f.write(str(test_texts[i]) + \"\\t\" + str(test_labels[i]) + \"\\t\" + str(results[i]) + \"\\n\")\n",
        "\n",
        "f.write(\"number of correct: \" + str(count) + \" out of \" + str(len(test_labels)) + \"\\n\")\n",
        "\n",
        "f.close()\n",
        "print(\"accuracy =\",count/len(test_labels),\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix : \n",
            "[[ 687   10    7  124]\n",
            " [   7  963   30  125]\n",
            " [   7   57  858  165]\n",
            " [ 135   94   88 2674]]\n",
            "classification_report: \n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.82      0.83      0.83       828\n",
            "          1       0.86      0.86      0.86      1125\n",
            "          2       0.87      0.79      0.83      1087\n",
            "          3       0.87      0.89      0.88      2991\n",
            "\n",
            "avg / total       0.86      0.86      0.86      6031\n",
            "\n",
            "accuracy = 0.8592273254849941 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI1XpvKCCmZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data = pd.read_csv(\"devwithoutlabels.txt\",sep='\\t')\n",
        "final_data['single'] = final_data['turn1']+\" \"+final_data['turn2']+\" \"+final_data['turn3']\n",
        "# drop_features(['id','turn1','turn2','turn3'],test_data)\n",
        "final_text = []\n",
        "for i in list(final_data['single']):\n",
        "    final_text.append(re.sub(r'[-()_,.:@#?!&$]', ' ', emoji.demojize(i)))\n",
        "final_data_sequences = sequence.pad_sequences( tokenizer.texts_to_sequences( final_text ) , maxlen=max_sent_len )\n",
        "test_results = model.predict_classes( final_data_sequences )\n",
        "test_results = list(test_results)\n",
        "for i in range(len(test_results)):\n",
        "    if test_results[i] == 0:\n",
        "        test_results[i]= \"happy\"\n",
        "    elif test_results[i] == 1:\n",
        "        test_results[i]= \"sad\"\n",
        "    elif test_results[i] == 2:\n",
        "        test_results[i]= \"angry\"\n",
        "    elif test_results[i] == 3:\n",
        "        test_results[i]= \"others\" \n",
        "final_result = pd.DataFrame({'turn1':final_data['turn1'],'turn2':final_data['turn2'],'turn3':final_data['turn3']})\n",
        "final_result['label'] = test_results\n",
        "final_result.index.names=['id']\n",
        "final_result.to_csv('test.txt',sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}